{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AVJdataminer/COVID19_GC/blob/master/COVID19_GuidedCapstoneStep4andStep5_AnswerKey.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cf5CmXQCZyF1"
   },
   "source": [
    "# Guided Capstone Step 4. Pre-processing and Training Data Development - Answer Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbZXsVevfr9M"
   },
   "source": [
    "**The Data Science Method**  \n",
    "\n",
    "\n",
    "1.   Problem Identification \n",
    "\n",
    "\n",
    "2.   Data Wrangling \n",
    "  \n",
    " \n",
    "3.   Exploratory Data Analysis   \n",
    "\n",
    "4.   **Pre-processing and Training Data Development**  \n",
    " * Create dummy or indicator features for categorical variables\n",
    "  * Standardize the magnitude of numeric features\n",
    "  * Split into testing and training datasets\n",
    "  * Apply scaler to the testing set\n",
    "5.   Modeling \n",
    "  * Fit Models with Training Data Set\n",
    "  * Review Model Outcomes â€” Iterate over additional models as needed.\n",
    "  * Identify the Final Model\n",
    "\n",
    "6.   Documentation\n",
    "  * Review the Results\n",
    "  * Present and share your findings - storytelling\n",
    "  * Finalize Code \n",
    "  * Finalize Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8xfkAqqZyF2"
   },
   "source": [
    "**<font color='DarkBlue'> Start by loading the necessary packages as we did in step 3 and printing out our current working directory just to confirm we are in the correct project directory. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ry6WPL5eZyF3",
    "outputId": "cd62142f-2ecf-49c0-8667-1571e000a058"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f971b0fe11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "import plotly.graph_objects as go \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "630T-ogRZyF8"
   },
   "source": [
    "**<font color='DarkBlue'> If you need to change your path refer back to the notebook on steps 1 & 2 on how to do that. Then load the csv file you created in step 3, remember it should be saved inside your data subfolder and print the first five rows.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "colab_type": "code",
    "id": "dMNbk0u3ZyF9",
    "outputId": "021a0303-ff00-4a51-d0da-d196e2cb3e3b"
   },
   "outputs": [],
   "source": [
    "file='https://raw.githubusercontent.com/AVJdataminer/COVID19_GC/master/%20data/step3_output.csv'\n",
    "df=pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "UI0DRGj9kauE",
    "outputId": "48990a5c-dbab-4a80-a892-79ff82f15de4"
   },
   "outputs": [],
   "source": [
    "ds = df.groupby(['timestamp.date']).agg({'confirmed':'sum','deaths':'sum', 'recovered':'sum'}).reset_index()\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "LiTtMkb_kx8H",
    "outputId": "8e0f2b4e-07ad-42d9-f282-1fa1520f6a50"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                x=ds['timestamp.date'],\n",
    "                y=ds['confirmed'],\n",
    "                name=\"confirmed\",\n",
    "                line_color='red',\n",
    "                opacity=0.8))\n",
    "fig.add_trace(go.Scatter(\n",
    "                x=ds['timestamp.date'],\n",
    "                y=ds['deaths'],\n",
    "                name=\"deaths\",\n",
    "                line_color='dimgray',\n",
    "                opacity=0.8))\n",
    "fig.add_trace(go.Scatter(\n",
    "                x=ds['timestamp.date'],\n",
    "                y=ds['recovered'],\n",
    "                name=\"recovered\",\n",
    "                line_color='green',\n",
    "                opacity=0.8))\n",
    "\n",
    "# Use date string to set xaxis range\n",
    "fig.update_layout(xaxis_range=['2020-01-22','2020-04-22'],\n",
    "                  title_text=\"COVID-19 US Confirmed Cases\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkBHf9smZyGB"
   },
   "source": [
    "## Create dummy features for categorical variables - when applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hC2fyC4hi6MD"
   },
   "source": [
    "**<font color='DarkBlue'> Check the values for `province_state` and determine if dummies need to be created, if so, add the dummies back to the dataframe and remove the original column for `province_state`. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1wkyECMOcAcQ",
    "outputId": "5a5c3b5e-930b-446f-de3c-d4d924d6b832"
   },
   "outputs": [],
   "source": [
    "df.province_state.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr_eYIn7ctWN"
   },
   "source": [
    "**<font color='DarkBlue'> Currently there are no states in this dataset so we skip this step. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZqWk8ltZyGZ"
   },
   "outputs": [],
   "source": [
    "#df = pd.concat([df, pd.get_dummies(df['province_state'])], axis=1).drop(['province_state'], axis =1)\n",
    "#print(df.shape)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HnDVhE1-ZyGF"
   },
   "source": [
    "## Standardize the magnitude of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BADoC4UAc--X"
   },
   "source": [
    "**<font color='DarkBlue'> In the last step you may remember we applied a scaler to our data before fitting the Lasso Regression, however, we didn't save that in the output data so we will need to apply that step again before modeling the US data. Additionally, we need use the simple imputer to fill the null values once again. Start by filling the null values than apply the scaler to the filled numpy array. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IZL-q-KtAYI6"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "response ='confirmed'\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = df.drop([response], axis=1)._get_numeric_data()\n",
    "imputer=imp.fit(X)\n",
    "X_filled=imputer.transform(X)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_filled)\n",
    "X_scaled=scaler.transform(X_filled)\n",
    "y = df[[response]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAT8h4_mZyGK"
   },
   "source": [
    "## Split into training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zDMyE5ui6Mx"
   },
   "source": [
    "**<font color='DarkBlue'> Split the data into training and testing data subset based on date.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSkPut0gguds"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "def dt_splitter(date_col, X, y, test_size):\n",
    "        date_col = pd.to_datetime(date_col)\n",
    "        xw_date=pd.DataFrame(X).merge(date_col,left_index=True, right_index=True)\n",
    "        ad = (max(xw_date.date)- min(xw_date.date)).days*test_size\n",
    "        split_date = min(xw_date.date) + timedelta(days=ad)\n",
    "        X_train = xw_date.loc[xw_date['date'] <= split_date].drop(['date'], axis=1).values\n",
    "        X_test = xw_date.loc[xw_date['date'] > split_date].drop(['date'], axis=1).values\n",
    "        yw_date=pd.DataFrame(y).merge(date_col,left_index=True, right_index=True)\n",
    "        y_train=yw_date.loc[yw_date['date'] <= split_date].drop(['date'], axis=1).values\n",
    "        y_test=yw_date.loc[yw_date['date'] > split_date].drop(['date'], axis=1).values\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test=dt_splitter(df['date'], X_scaled, y, .80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1JBBCMXni6Nt",
    "outputId": "e2c779aa-13af-40a3-c4d4-3b32be093a33"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jz8Jnu6Zd4tW",
    "outputId": "e95c4c4a-21f8-413f-aae1-f44337ab85eb"
   },
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score,mean_absolute_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import explained_variance_score\n",
    "lassoreg = Lasso(alpha=1,normalize=True, max_iter=1e5)\n",
    "lassoreg.fit(X_train,y_train)\n",
    "y_pred = lassoreg.predict(X_test)\n",
    "print('Mean explained variance score for confirmed cases for the testing period =%.3f' % explained_variance_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "nvS9M5PTeDjG",
    "outputId": "5a692ab6-5186-4454-c01b-79fc584043f0"
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_pred,y_test)\n",
    "plt.plot([x for x in range(0,150000)],[x for x in range(0,150000)], color='red')\n",
    "plt.title(\"Model y predicted by actuals\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGPk6KXVZjjX"
   },
   "source": [
    "Model Confirmed cases with timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "eZ6gjEE9i6Nw",
    "outputId": "92eb4e75-526c-493d-f9fe-138af19c9523"
   },
   "outputs": [],
   "source": [
    "#create timeseries data, so only date and confirmed cases data frame.\n",
    "cdf = df[['date', 'confirmed']]\n",
    "cdf['date'] = pd.to_datetime(cdf['date'])\n",
    "cdf.set_index('date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "7chBiCqsBDsD",
    "outputId": "ce242676-c9a9-4354-e313-0b63c82bc66c"
   },
   "outputs": [],
   "source": [
    "y = cdf['confirmed']\n",
    "y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "sokeBucQZnCv",
    "outputId": "91c7dffd-8189-4de3-d692-33f47f56d719"
   },
   "outputs": [],
   "source": [
    "# Import seasonal_decompose \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Make a variable called decomposition\n",
    "decomposition = seasonal_decompose(y,freq=30)\n",
    "\n",
    "# Make three variables for trend, seasonal and residual components respectively. \n",
    "# Assign them the relevant features of decomposition \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plot the original data, the trend, the seasonality, and the residuals \n",
    "plt.subplot(411)\n",
    "plt.plot(y, label = 'Original')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label = 'Trend')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label = 'Seasonality')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label = 'Residuals')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "okdKO9QiDDLj",
    "outputId": "8030152c-4236-46ba-a022-404fc6adabdf"
   },
   "outputs": [],
   "source": [
    "#testing for stationarity\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "kpss(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2QPle0QtDJ7c"
   },
   "source": [
    "Since our p-value is less than 0.05, we should reject the Null hypothesis and deduce the non-stationarity of our data. \n",
    "\n",
    "But our data need to be stationary! So we need to do some transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0K2kJWrCx7Z"
   },
   "outputs": [],
   "source": [
    "# Import mean_squared_error and ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v85lWaniD6SZ"
   },
   "outputs": [],
   "source": [
    "# Make a function to find the MSE of a single ARIMA model \n",
    "def evaluate_arima_model(data, arima_order):\n",
    "    # Needs to be an integer because it is later used as an index.\n",
    "    split=int(len(data) * 0.8) \n",
    "    train, test = data[0:split], data[split:len(data)]\n",
    "    past=[x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):#timestep-wise comparison between test data and one-step prediction ARIMA model. \n",
    "        model = ARIMA(past, order=arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        future = model_fit.forecast()[0]\n",
    "        predictions.append(future)\n",
    "        past.append(test[i])\n",
    "    # calculate out of sample error\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kxvgh50D857"
   },
   "outputs": [],
   "source": [
    "# Make a function to evaluate different ARIMA models with several different p, d, and q values.\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(dataset, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    return print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xP7gKO6EAF1"
   },
   "outputs": [],
   "source": [
    "# Now, we choose a couple of values to try for each parameter.\n",
    "p_values = [x for x in range(0, 4)]\n",
    "d_values = [x for x in range(0, 1)]\n",
    "q_values = [x for x in range(15, 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6OtUtVDLECol",
    "outputId": "14e7de9a-30ea-40d8-ad02-d6a438880f72"
   },
   "outputs": [],
   "source": [
    "# Finally, we can find the optimum ARIMA model for our data.\n",
    "# Nb. this can take a while...!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "y_log = np.log(y)\n",
    "evaluate_models(y_log, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n66asbHiEGvP"
   },
   "outputs": [],
   "source": [
    "2p=0\n",
    "d=1\n",
    "q=2\n",
    "model = ARIMA(y_log, order=(p,d,q))\n",
    "model_fit = model.fit()\n",
    "forecast = model_fit.forecast(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "Ok9Z5tUcEbm0",
    "outputId": "ae9869bd-93a7-408c-d695-32855dd00669"
   },
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "Zut6705gOtEO",
    "outputId": "eb249d1a-4a8e-4a52-8e12-a644a2da22b2"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(cdf.confirmed, order=(0,1,2))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "print('Residuals Description')\n",
    "print(model_fit.resid.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "xCvCOhYhEfAL",
    "outputId": "f0783862-32a8-4751-fa8c-6303f9598d65"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(y_log.diff())\n",
    "plt.plot(model_fit.predict(), color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "16eEIDUhPiLV",
    "outputId": "8534df9f-5a4e-4393-c5e5-929b196042b6"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "Wj0fzy7DTjZm",
    "outputId": "2c121e79-02e2-4b92-9d8d-a547c3573481"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "#calling auto correlation function\n",
    "lag_acf = acf(y, nlags=300)\n",
    "#Plot PACF:\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(lag_acf,marker='+')\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(y)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(y)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "plt.xlabel('number of lags')\n",
    "plt.ylabel('correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "colab_type": "code",
    "id": "gfXyM4oLG-n9",
    "outputId": "ba318937-f959-4b10-dcb6-c1b17598a1fc"
   },
   "outputs": [],
   "source": [
    "\n",
    "#calling partial correlation function\n",
    "lag_pacf = pacf(y, nlags=30, method='ols')\n",
    "#Plot PACF:\n",
    "plt.figure(figsize=(16, 7))\n",
    "plt.plot(lag_pacf,marker='+')\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(y)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(y)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.xlabel('Number of lags')\n",
    "plt.ylabel('correlation')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjOF-lDHTZY7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "COVID19_GuidedCapstoneStep4andStep5-AnswerKey.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}